{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall opencv-contrib-python opencv-python\n",
    "# %pip install numpy opencv --user\n",
    "# %pip install centroid-tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necesarry packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Minimum probability so that an image can be classified as 'confident' \n",
    "# and thus identified\n",
    "confidence = 0.5\n",
    "# Non-maxima suppresion threshold, solves the problem of having multiple detections \n",
    "# for the same object by applying this value which reduces it to one detection for a \n",
    "# valid object \n",
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of labels\n",
    "labels = open('./yolo-coco/coco.names').read().strip().split('\\n')\n",
    "\n",
    "# Get the weights file\n",
    "weights = './yolo-coco/yolov3.weights'\n",
    "# Get the config file for the pre-trained model\n",
    "configFile = './yolo-coco/yolov3.cfg'\n",
    "\n",
    "# Load the YOLO object trained on the COCO dataset\n",
    "network = cv2.dnn.readNetFromDarknet(configFile, weights)\n",
    "\n",
    "# Get the layer names\n",
    "layerNames = network.getLayerNames()\n",
    "layerNamesOut = [layerNames[i - 1] for i in network.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image (run this cell if image tracking is required)\n",
    "image = cv2.imread('./output_video_static_Moment.jpg')\n",
    "# Get image dimensions\n",
    "height, width = image.shape[:2]\n",
    "# Create a blob with respective parameters (check docs for more info)\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), (0,0,0), \n",
    "                             swapRB=True, crop=False)\n",
    "network.setInput(blob)\n",
    "# Do a forward pass using the YOLO object detector (outputs \n",
    "# bounding boxes with respective probabilities)\n",
    "layerOut = network.forward(layerNamesOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results output is: [38 32  6 35 30 24 29 17 12 10 16 15  5 27  4  2  3  0]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists which will contain: bounding boxes, confidence \n",
    "# and classID related by array index\n",
    "boundingBoxes = []\n",
    "confidenceList = []\n",
    "classIDs = []\n",
    "\n",
    "# Iterate over each layer output\n",
    "for out in layerOut:\n",
    "    # Iterate over each detection+\n",
    "    for detection in out:\n",
    "        # Get score, classID and condifence\n",
    "        scores = detection[5:]\n",
    "        currClass = np.argmax(scores)\n",
    "        currConfidence = scores[currClass]\n",
    "\n",
    "        # Check image confidence using value establashied previously\n",
    "        if currConfidence > confidence: \n",
    "            # Scale bounding box relative to image size\n",
    "            # The way YOLO formats its data is by returning the center coordinates of the bounding box\n",
    "            # followed by the width and height, so with that in mind we apply it to our bonding box\n",
    "            box = detection[0:4] * np.array([width, height, width, height])\n",
    "            xCenter, yCenter, boxWidth, boxHeight = box.astype('int')\n",
    "\n",
    "            # Use center coordinates to calculate top-left box corner coordinate (same as Android)\n",
    "            x = int(xCenter - boxWidth / 2)\n",
    "            y = int(yCenter - boxHeight / 2)\n",
    "\n",
    "            # Add values to the lists\n",
    "            boundingBoxes.append([x, y, int(boxWidth), int(boxHeight)])\n",
    "            classIDs.append(currClass)\n",
    "            confidenceList.append(float(currConfidence))\n",
    "\n",
    "# Apply non-maxima suppresion threshold\n",
    "results = cv2.dnn.NMSBoxes(boundingBoxes, confidenceList, confidence, threshold)\n",
    "print(\"Results output is: {}\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.63117892e+00 -3.43119474e+01  1.65517083e+04]\n",
      " [ 3.24112855e+00 -5.95395319e+01  1.42339557e+04]\n",
      " [ 4.40221099e-04 -3.78973971e-02  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "footballFieldImage = cv2.imread('./football_field.jpg')\n",
    "\n",
    "# Real field coordinates\n",
    "src = np.array([[1145, 310], [1915, 353], [1915, 1075], [1, 510]], np.float32)\n",
    "\n",
    "'''\n",
    "cv2.circle(image, (1145, 310), 20, (0, 0, 255), -2,)  # Upper left\n",
    "cv2.circle(image, (1915, 353), 20, (0, 0, 255), -2,)  # Upper right\n",
    "cv2.circle(image, (1915, 1075), 20, (0, 0, 255), -2,)  # Lower right\n",
    "cv2.circle(image, (1, 510), 20, (0, 0, 255), -2,)  # Lower left\n",
    "\n",
    "ims = cv2.resize(image, (960, 540))\n",
    "cv2.imshow(\"Tracked players\", ims)\n",
    "cv2.waitKey(0)\n",
    "'''\n",
    "\n",
    "# Cartoon image field coordinates\n",
    "dst = np.array([[52, 50], [550, 50], [800, 1120], [52, 880]], np.float32)\n",
    "\n",
    "'''\n",
    "cv2.circle(footballFieldImage, (52, 50), 15, (0, 0, 255), -2,)  # Upper left \n",
    "cv2.circle(footballFieldImage, (550, 50), 15, (0, 0, 255), -2,)  # Upper right \n",
    "cv2.circle(footballFieldImage, (800, 1120), 15, (0, 0, 255), -2,)  # Lower right\n",
    "cv2.circle(footballFieldImage, (52, 880), 15, (0, 0, 255), -2,)  # Lower left \n",
    "'''\n",
    "\n",
    "# Show output image\n",
    "ims = cv2.resize(footballFieldImage, (960, 540))\n",
    "cv2.imshow(\"Tracked players\", ims)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Get the perspective transformation matrix (x and y are the centroid coordinates)\n",
    "pmatrix = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "print(pmatrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw the boxes on the image and its respective text\n",
    "\n",
    "# Create a list of random colors\n",
    "np.random.seed(21)\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "# Make sure we have results on the analysis\n",
    "if len(results) > 0:\n",
    "    # Iterate over list indices\n",
    "    for i in results:\n",
    "        # Get bounding box coordinates\n",
    "        x, y = boundingBoxes[i][0], boundingBoxes[i][1]\n",
    "        w, h = boundingBoxes[i][2], boundingBoxes[i][3]\n",
    "\n",
    "        # Draw bounding box with its label overlayed on the image\n",
    "        currentBoxColour = [int(j) for j in colours[classIDs[i]]]\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), currentBoxColour, 2)\n",
    "        currentBoxText = '{}: {:.3f}'.format(labels[classIDs[i]], confidenceList[i])\n",
    "        cv2.putText(image, currentBoxText, (x, y - 6), cv2.FONT_HERSHEY_TRIPLEX, 0.5, currentBoxColour, 2)\n",
    "\n",
    "        # Get projected coordinates on 2d field\n",
    "        centerX, centerY = (x + w/2), (y + h/2)\n",
    "        # print(\"Original coordinates:\\n {}\\n\".format(np.array([centerX, centerY+h/4, 1])))\n",
    "        val = np.dot(pmatrix, np.array([centerX, centerY+h/4, 1]))\n",
    "        # print(\"Result coordinates after multiplication: \\n{}\\n\".format(val))\n",
    "        x2d, y2d = val[0]/val[2], val[1]/val[2]\n",
    "        # print(\"Final coordinates ({}, {})\".format(x2d, y2d))\n",
    "        cv2.circle(footballFieldImage, (int(x2d), int(y2d)), 10, (255, 255, 0), -2)\n",
    "\n",
    "\n",
    "# Show output image\n",
    "ims = cv2.resize(image, (960, 540))\n",
    "cv2.imshow(\"Tracked players\", ims)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "ims2 = cv2.resize(footballFieldImage, (960, 540))\n",
    "cv2.imshow(\"Tracked players\", ims2)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necesarry packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tracker\n",
    "\n",
    "# Minimum probability so that an image can be classified as 'confident' \n",
    "# and thus identified\n",
    "confidence = 0.5\n",
    "# Non-maxima suppresion threshold, solves the problem of having multiple \n",
    "# detections for the same object by applying this value which reduces it \n",
    "# to one detection for a valid object \n",
    "threshold = 0.3\n",
    "# Initialize centroid tracker which will track a specific player and allow \n",
    "# us to assign an ID to it\n",
    "ctracker = tracker.CentroidTracker(maxDisappeared=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of labels\n",
    "labels = open('./yolo-coco/coco.names').read().strip().split('\\n')\n",
    "\n",
    "# Get the weights file\n",
    "weights = './yolo-coco/yolov3.weights'\n",
    "# Get the config file for the pre-trained model\n",
    "configFile = './yolo-coco/yolov3.cfg'\n",
    "\n",
    "# Load the YOLO object trained on the COCO dataset\n",
    "network = cv2.dnn.readNetFromDarknet(configFile, weights)\n",
    "\n",
    "# Get the layer names\n",
    "layerNames = network.getLayerNames()\n",
    "layerNamesOut = [layerNames[i - 1] for i in network.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of frames: 75\n"
     ]
    }
   ],
   "source": [
    "# Get video (run this cell if video tracking is required)\n",
    "# Change value depending on short or long vid\n",
    "shortVid = True \n",
    "if shortVid:\n",
    "    # video = cv2.VideoCapture('videos/madrid_psg_short.mp4')\n",
    "    # outputVideo = 'output_video_short.mp4'\n",
    "    video = cv2.VideoCapture('videos/madrid_psg_static.mp4')\n",
    "    outputVideo = 'output_video_static.mp4'\n",
    "else:\n",
    "    video = cv2.VideoCapture('videos/madrid_psg.mp4')\n",
    "    outputVideo = 'output_video.mp4'\n",
    "\n",
    "writer = None\n",
    "width, height = None, None\n",
    "total = 0\n",
    "\n",
    "# Get total no. of frames in the video\n",
    "try: \n",
    "    total = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "except:\n",
    "    print(\"Couldn't get frame count\")\n",
    "\n",
    "print('No. of frames: {}'.format(total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real field coordinates\n",
    "src = np.array([[1145, 310], [1915, 353], [1915, 1075], [1, 510]], np.float32)\n",
    "# Cartoon image field coordinates\n",
    "dst = np.array([[52, 50], [550, 50], [800, 1120], [52, 880]], np.float32)\n",
    "\n",
    "# Get the perspective transformation matrix (x and y are the centroid coordinates)\n",
    "pmatrix = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "# Define output video\n",
    "outputBEV = 'output_birds_eye_view.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyYoloToFrame(frame):\n",
    "    # Create a blob with respective parameters (check docs for more info)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "    network.setInput(blob)\n",
    "\n",
    "    # Do a forward pass using the YOLO object detector (outputs bounding boxes with respective probabilities)\n",
    "    layerOut = network.forward(layerNamesOut)\n",
    "\n",
    "    # Initialize lists which will contain: bounding boxes, confidence and classID related by array index\n",
    "    boundingBoxes = []\n",
    "    confidenceList = []\n",
    "    classIDs = []\n",
    "\n",
    "    # Iterate over each layer output\n",
    "    for out in layerOut:\n",
    "        # Iterate over each detection+\n",
    "        for detection in out:\n",
    "            # Get score, classID and condifence\n",
    "            scores = detection[5:]\n",
    "            currClass = np.argmax(scores)\n",
    "            currConfidence = scores[currClass]\n",
    "\n",
    "            # Check image confidence using value establashied previously\n",
    "            if currConfidence > confidence: \n",
    "                # Scale bounding box relative to image size\n",
    "                # The way YOLO formats its data is by returning the center coordinates of the bounding box\n",
    "                # followed by the width and height, so with that in mind we apply it to our bonding box\n",
    "                box = detection[0:4] * np.array([width, height, width, height])\n",
    "                xCenter, yCenter, boxWidth, boxHeight = box.astype('int')\n",
    "\n",
    "                # Use center coordinates to calculate top-left box corner coordinate (same as Android)\n",
    "                x = int(xCenter - (boxWidth / 2))\n",
    "                y = int(yCenter - (boxHeight / 2))\n",
    "\n",
    "                # Add values to the lists\n",
    "                boundingBoxes.append(np.array([x, y, int(boxWidth), int(boxHeight)]))\n",
    "                classIDs.append(currClass)\n",
    "                confidenceList.append(float(currConfidence))\n",
    "\n",
    "    # Apply non-maxima suppresion threshold\n",
    "    rects = cv2.dnn.NMSBoxes(boundingBoxes, confidenceList, confidence, threshold)\n",
    "    objects = ctracker.update([boundingBoxes[idx] for idx in rects])\n",
    "\n",
    "    # Draw the boxes on the image and its respective text\n",
    "    # Create a list of random colors\n",
    "    np.random.seed(21)\n",
    "    colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "    # Make sure we have results on the analysis\n",
    "    for (id, bbox) in objects[1].items():\n",
    "        # Get bounding box coordinates\n",
    "        x, y, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "        # Draw bounding box with its label overlayed on the image\n",
    "        currentBoxColour = (255, 0, 0) # Blue\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), currentBoxColour, 2)\n",
    "        currentBoxText = 'Current ID: {}'.format(id)\n",
    "        cv2.putText(frame, currentBoxText, (x, y - 6), cv2.FONT_HERSHEY_TRIPLEX, 0.5, currentBoxColour, 2)\n",
    "\n",
    "        # Get projected coordinates on 2d field\n",
    "        centerX, centerY = (x + w/2), (y + h/2)\n",
    "        # print(\"Original coordinates:\\n {}\\n\".format(np.array([centerX, centerY+h/4, 1])))\n",
    "        val = np.dot(pmatrix, np.array([centerX, centerY+h/4, 1]))\n",
    "        # print(\"Result coordinates after multiplication: \\n{}\\n\".format(val))\n",
    "        x2d, y2d = val[0]/val[2], val[1]/val[2]\n",
    "        # print(\"Result coordinates ({}, {})\".format(x2d, y2d))\n",
    "        cv2.circle(footballFieldImage, (int(x2d), int(y2d)), 10, (0, 0, 0), -2)\n",
    "        cv2.putText(footballFieldImage, str(id), (int(x2d), int(y2d) - 14), cv2.FONT_HERSHEY_TRIPLEX, 0.5, (0, 0, 0), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current frame no: 0\n",
      "Current frame no: 1\n",
      "Current frame no: 2\n",
      "Current frame no: 3\n",
      "Current frame no: 4\n",
      "Current frame no: 5\n",
      "Current frame no: 6\n",
      "Current frame no: 7\n",
      "Current frame no: 8\n",
      "Current frame no: 9\n",
      "Current frame no: 10\n",
      "Current frame no: 11\n",
      "Current frame no: 12\n",
      "Current frame no: 13\n",
      "Current frame no: 14\n",
      "Current frame no: 15\n",
      "Current frame no: 16\n",
      "Current frame no: 17\n",
      "Current frame no: 18\n",
      "Current frame no: 19\n",
      "Current frame no: 20\n",
      "Current frame no: 21\n",
      "Current frame no: 22\n",
      "Current frame no: 23\n",
      "Current frame no: 24\n",
      "Current frame no: 25\n",
      "Current frame no: 26\n",
      "Current frame no: 27\n",
      "Current frame no: 28\n",
      "Current frame no: 29\n",
      "Current frame no: 30\n",
      "Current frame no: 31\n",
      "Current frame no: 32\n",
      "Current frame no: 33\n",
      "Current frame no: 34\n",
      "Current frame no: 35\n",
      "Current frame no: 36\n",
      "Current frame no: 37\n",
      "Current frame no: 38\n",
      "Current frame no: 39\n",
      "Current frame no: 40\n",
      "Current frame no: 41\n",
      "Current frame no: 42\n",
      "Current frame no: 43\n",
      "Current frame no: 44\n",
      "Current frame no: 45\n",
      "Current frame no: 46\n",
      "Current frame no: 47\n",
      "Current frame no: 48\n",
      "Current frame no: 49\n",
      "Current frame no: 50\n",
      "Current frame no: 51\n",
      "Current frame no: 52\n",
      "Current frame no: 53\n",
      "Current frame no: 54\n",
      "Current frame no: 55\n",
      "Current frame no: 56\n",
      "Current frame no: 57\n",
      "Current frame no: 58\n",
      "Current frame no: 59\n",
      "Current frame no: 60\n",
      "Current frame no: 61\n",
      "Current frame no: 62\n",
      "Current frame no: 63\n",
      "Current frame no: 64\n",
      "Current frame no: 65\n",
      "Current frame no: 66\n",
      "Current frame no: 67\n",
      "Current frame no: 68\n",
      "Current frame no: 69\n",
      "Current frame no: 70\n",
      "Current frame no: 71\n",
      "Current frame no: 72\n",
      "Current frame no: 73\n",
      "Current frame no: 74\n",
      "Current frame no: 75\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each frame\n",
    "num = 0\n",
    "while True:\n",
    "    # Get next frame\n",
    "    pickedFrame, frame = video.read()\n",
    "    print(\"Current frame no: {}\".format(num))\n",
    "    num += 1\n",
    "\n",
    "    # Check if there are frames left to analyze\n",
    "    if not pickedFrame:\n",
    "        break\n",
    "\n",
    "    # Get football field frame\n",
    "    footballFieldImage = cv2.imread('./football_field.jpg')\n",
    "\n",
    "    # Same procedure as with image tracking\n",
    "    height, width = frame.shape[:2] \n",
    "\n",
    "    # Call function\n",
    "    applyYoloToFrame(frame)\n",
    "\n",
    "    # Initialize writer\n",
    "    if writer is None:\n",
    "        # Concatenate four chars to generate a fourcc code\n",
    "        fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "        # Params are: filename, apiPreference, fourcc, fps, frameSize, isColor\n",
    "        writer = cv2.VideoWriter(outputVideo, fourcc, 30, \n",
    "                                (frame.shape[1], frame.shape[0]), True)\n",
    "        writerBEV = cv2.VideoWriter(outputBEV, fourcc, 30, (footballFieldImage.shape[1], \n",
    "                                    footballFieldImage.shape[0]), True)\n",
    "\n",
    "    # Write the output\n",
    "    writer.write(frame)\n",
    "    writerBEV.write(footballFieldImage)\n",
    "\n",
    "# Release pointers (good practice)\n",
    "writer.release()\n",
    "writerBEV.release()\n",
    "video.release() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birds-eye view"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc3a6c061aee08cc13f69f101dbca054e4ee7755b6859314673b45228bf6bb05"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
